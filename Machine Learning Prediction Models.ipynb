{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "l6RW4TkF0eqK",
        "X_TYKFg60meT",
        "gcqTUMf60tXn",
        "_X7q6jYH_a3T",
        "yf82lOBh07P3",
        "pYQS57bueEA-",
        "GuJKWzq0PA7y",
        "JAwLlbmI9U3m"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirement.txt"
      ],
      "metadata": {
        "id": "fGvH26CF58cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from lightgbm import LGBMRegressor\n",
        "import sklearn\n",
        "from sklearn import tree\n",
        "from sklearn import svm\n",
        "from sklearn import neighbors\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "import pickle"
      ],
      "metadata": {
        "id": "ptjHqpfQOvUa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('./models')"
      ],
      "metadata": {
        "id": "l0QIXduIkXCw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PREPROCESSING**"
      ],
      "metadata": {
        "id": "l6RW4TkF0eqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FLIR_dataset:\n",
        "\n",
        "  def preprocessing(self,filename):\n",
        "    lst1 = ['SubjectID','T_offset_1', 'Max1R13_1', 'Max1L13_1', 'aveAllR13_1', 'aveAllL13_1', 'T_RC_1', 'T_RC_Dry_1', 'T_RC_Wet_1', 'T_RC_Max_1', 'T_LC_1', 'T_LC_Dry_1', 'T_LC_Wet_1', 'T_LC_Max_1', 'RCC_1', 'LCC_1', 'canthiMax_1', 'canthi4Max_1', 'T_FHCC_1', 'T_FHRC_1', 'T_FHLC_1', 'T_FHBC_1', 'T_FHTC_1', 'T_FH_Max_1', 'T_FHC_Max_1', 'T_Max_1', 'T_OR_1', 'T_OR_Max_1','Gender', 'Age', 'Ethnicity','T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
        "    lst2 = ['SubjectID','T_offset_2', 'Max1R13_2', 'Max1L13_2', 'aveAllR13_2', 'aveAllL13_2', 'T_RC_2', 'T_RC_Dry_2', 'T_RC_Wet_2', 'T_RC_Max_2', 'T_LC_2', 'T_LC_Dry_2', 'T_LC_Wet_2', 'T_LC_Max_2', 'RCC_2', 'LCC_2', 'canthiMax_2', 'canthi4Max_2', 'T_FHCC_2', 'T_FHRC_2', 'T_FHLC_2', 'T_FHBC_2', 'T_FHTC_2', 'T_FH_Max_2', 'T_FHC_Max_2', 'T_Max_2', 'T_OR_2', 'T_OR_Max_2','Gender', 'Age', 'Ethnicity','T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
        "    lst3 = ['SubjectID','T_offset_3', 'Max1R13_3', 'Max1L13_3', 'aveAllR13_3', 'aveAllL13_3', 'T_RC_3', 'T_RC_Dry_3', 'T_RC_Wet_3', 'T_RC_Max_3', 'T_LC_3', 'T_LC_Dry_3', 'T_LC_Wet_3', 'T_LC_Max_3', 'RCC_3', 'LCC_3', 'canthiMax_3', 'canthi4Max_3', 'T_FHCC_3', 'T_FHRC_3', 'T_FHLC_3', 'T_FHBC_3', 'T_FHTC_3', 'T_FH_Max_3', 'T_FHC_Max_3', 'T_Max_3', 'T_OR_3', 'T_OR_Max_3','Gender', 'Age', 'Ethnicity','T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
        "    lst4 = ['SubjectID','T_offset_4', 'Max1R13_4', 'Max1L13_4', 'aveAllR13_4', 'aveAllL13_4', 'T_RC_4', 'T_RC_Dry_4', 'T_RC_Wet_4', 'T_RC_Max_4', 'T_LC_4', 'T_LC_Dry_4', 'T_LC_Wet_4', 'T_LC_Max_4', 'RCC_4', 'LCC_4', 'canthiMax_4', 'canthi4Max_4', 'T_FHCC_4', 'T_FHRC_4', 'T_FHLC_4', 'T_FHBC_4', 'T_FHTC_4', 'T_FH_Max_4', 'T_FHC_Max_4', 'T_Max_4', 'T_OR_4', 'T_OR_Max_4','Gender', 'Age', 'Ethnicity','T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
        "\n",
        "    lst5 = ['T_offset_1', 'Max1R13_1', 'Max1L13_1', 'aveAllR13_1', 'aveAllL13_1', 'T_RC_1', 'T_RC_Dry_1', 'T_RC_Wet_1', 'T_RC_Max_1', 'T_LC_1', 'T_LC_Dry_1', 'T_LC_Wet_1', 'T_LC_Max_1', 'RCC_1', 'LCC_1', 'canthiMax_1', 'canthi4Max_1', 'T_FHCC_1', 'T_FHRC_1', 'T_FHLC_1', 'T_FHBC_1', 'T_FHTC_1', 'T_FH_Max_1', 'T_FHC_Max_1', 'T_Max_1', 'T_OR_1', 'T_OR_Max_1']\n",
        "    lst6 = [\"_\".join(col.split('_')[:-1]) + '_2' for col in lst5]\n",
        "    lst7 = [\"_\".join(col.split('_')[:-1]) + '_3' for col in lst5]\n",
        "    lst8 =  [\"_\".join(col.split('_')[:-1]) + '_4' for col in lst5]\n",
        "\n",
        "    def column_change(lst1):\n",
        "      column1 = {}\n",
        "      for i in lst1:\n",
        "        if i[-1] in ('1','2','3','4'):\n",
        "          col1 = i[:-1]\n",
        "          column1[i] = col1\n",
        "\n",
        "      return column1\n",
        "\n",
        "    train = pd.read_csv(filename,skiprows=2)\n",
        "\n",
        "    col1 = []\n",
        "    for i in train.columns:\n",
        "      if train[i].isnull().all():\n",
        "        col1.append(i)\n",
        "\n",
        "    train1 = train.drop(columns = col1)\n",
        "\n",
        "    for col in lst5:\n",
        "      col1 = \"_\".join(col.split('_')[:-1])\n",
        "      train1[col1] = (train1[col1 + '_1'] + train1[col1 + '_2'] + train1[col1 + '_3'] + train1[col1 + '_4'])/4\n",
        "\n",
        "    train1.drop(columns = lst5 + lst6+lst7+lst8,inplace= True)\n",
        "    return train1\n",
        "\n",
        "  def feature_transformation(self,train1,mode):\n",
        "\n",
        "    num = []\n",
        "    cat = []\n",
        "\n",
        "    for col in train1.columns:\n",
        "\n",
        "      if train1[col].dtype == np.float64:\n",
        "        num.append(col)\n",
        "      else:\n",
        "        cat.append(col)\n",
        "\n",
        "    for col in num:\n",
        "      train1[col].fillna(train1[col].median(),inplace = True)\n",
        "\n",
        "    # print(train1.head())\n",
        "\n",
        "    if mode == 'train':\n",
        "      train1 = train1[train1[\"Distance\"] < 1]\n",
        "      train1 = train1[train1[\"T_offset\"] < (7/4)]\n",
        "      train1 = train1[train1[\"T_offset\"] > (1/4)]\n",
        "      train1 = train1[train1[\"T_FHCC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHRC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHLC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHLC\"] < (145/4)]\n",
        "      train1 = train1[train1[\"T_FHBC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHTC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHC_Max\"] > (132.5/4)]\n",
        "      train1 = train1[train1[\"T_FHC_Max\"] < (147.5/4)]\n",
        "      train1 = train1[train1[\"T_Max\"] < (150/4)]\n",
        "      train1.drop(columns = [\"T_offset\",\"T_atm\", \"Humidity\", \"Distance\"], inplace = True)\n",
        "      train1.reset_index(inplace = True)\n",
        "      train1.drop(columns = ['index'],inplace = True)\n",
        "\n",
        "    if mode == 'test':\n",
        "      if \"Cometics\" in train1.columns:\n",
        "        train1.drop(columns = ['Cosmetics'],inplace = True)\n",
        "\n",
        "\n",
        "    data_cat = train1[cat]\n",
        "    num1 = train1.copy(deep=True)\n",
        "\n",
        "    # data_cat.drop(columns = [\"SubjectID\"], inplace = True)\n",
        "    self.gender_dict = {'Female':1, 'Male':1}\n",
        "    self.age_dict = {'18-20':1, '21-25':1, '21-30':1, '26-30':1, '31-40':1, '41-50':1, '51-60':1, '>60':1}\n",
        "    self.ethnicity_dict = {'American Indian or Alaskan Native':1, 'Asian':1, 'Black or African-American':1, 'Hispanic/Latino':1, 'Multiracial':1, 'White':1}\n",
        "    for col in self.gender_dict:\n",
        "      values = []\n",
        "      for i in range(len(train1['Gender'])):\n",
        "        if train1['Gender'][i] == col:\n",
        "          values.append(1)\n",
        "        else:\n",
        "          values.append(0)\n",
        "      train1[col] = pd.Series(values)\n",
        "\n",
        "    for col in self.age_dict:\n",
        "      values = []\n",
        "      for i in range(len(train1['Age'])):\n",
        "        if train1['Age'][i] == col:\n",
        "          values.append(1)\n",
        "        else:\n",
        "          values.append(0)\n",
        "      train1[col] = pd.Series(values)\n",
        "\n",
        "    for col in self.ethnicity_dict:\n",
        "      values = []\n",
        "      for i in range(len(train1['Ethnicity'])):\n",
        "        if train1['Ethnicity'][i] == col:\n",
        "          values.append(1)\n",
        "        else:\n",
        "          values.append(0)\n",
        "      train1[col] = pd.Series(values)\n",
        "\n",
        "    drop_col = [\"SubjectID\" , \"Gender\", \"Age\" , \"Ethnicity\"]\n",
        "    train1.drop(columns = drop_col, inplace = True)\n",
        "    num1.drop(columns = drop_col,inplace = True)\n",
        "    return train1,num1\n"
      ],
      "metadata": {
        "id": "A9LZ6zZm4Tj8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ICI_dataset:\n",
        "  def preprocessing(self,filename):\n",
        "    lst1 = ['SubjectID','T_offset1', 'Max1R13_1', 'Max1L13_1', 'aveAllR13_1', 'aveAllL13_1', 'T_RC1', 'T_RC_Dry1', 'T_RC_Wet1', 'T_RC_Max1', 'T_LC1', 'T_LC_Dry1', 'T_LC_Wet1', 'T_LC_Max1', 'RCC1', 'LCC1', 'canthiMax1', 'canthi4Max1', 'T_FHCC1', 'T_FHRC1', 'T_FHLC1', 'T_FHBC1', 'T_FHTC1', 'T_FH_Max1', 'T_FHC_Max1', 'T_Max1', 'T_OR1', 'T_OR_Max1','Gender', 'Age', 'Ethnicity','T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
        "    lst2 = ['SubjectID','T_offset2', 'Max1R13_2', 'Max1L13_2', 'aveAllR13_2', 'aveAllL13_2', 'T_RC2', 'T_RC_Dry2', 'T_RC_Wet2', 'T_RC_Max2', 'T_LC2', 'T_LC_Dry2', 'T_LC_Wet2', 'T_LC_Max2', 'RCC2', 'LCC2', 'canthiMax2', 'canthi4Max2', 'T_FHCC2', 'T_FHRC2', 'T_FHLC2', 'T_FHBC2', 'T_FHTC2', 'T_FH_Max2', 'T_FHC_Max2', 'T_Max2', 'T_OR2', 'T_OR_Max2','Gender', 'Age', 'Ethnicity','T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
        "    lst3 = ['SubjectID','T_offset3', 'Max1R13_3', 'Max1L13_3', 'aveAllR13_3', 'aveAllL13_3', 'T_RC3', 'T_RC_Dry3', 'T_RC_Wet3', 'T_RC_Max3', 'T_LC3', 'T_LC_Dry3', 'T_LC_Wet3', 'T_LC_Max3', 'RCC3', 'LCC3', 'canthiMax3', 'canthi4Max3', 'T_FHCC3', 'T_FHRC3', 'T_FHLC3', 'T_FHBC3', 'T_FHTC3', 'T_FH_Max3', 'T_FHC_Max3', 'T_Max3', 'T_OR3', 'T_OR_Max3','Gender', 'Age', 'Ethnicity','T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
        "    lst4 = ['SubjectID','T_offset4', 'Max1R13_4', 'Max1L13_4', 'aveAllR13_4', 'aveAllL13_4', 'T_RC4', 'T_RC_Dry4', 'T_RC_Wet4', 'T_RC_Max4', 'T_LC4', 'T_LC_Dry4', 'T_LC_Wet4', 'T_LC_Max4', 'RCC4', 'LCC4', 'canthiMax4', 'canthi4Max4', 'T_FHCC4', 'T_FHRC4', 'T_FHLC4', 'T_FHBC4', 'T_FHTC4', 'T_FH_Max4', 'T_FHC_Max4', 'T_Max4', 'T_OR4', 'T_OR_Max4','Gender', 'Age', 'Ethnicity','T_atm', 'Humidity', 'Distance', 'aveOralM']\n",
        "\n",
        "    lst5 = ['T_offset1', 'Max1R131', 'Max1L131', 'aveAllR131', 'aveAllL131', 'T_RC1', 'T_RC_Dry1', 'T_RC_Wet1', 'T_RC_Max1', 'T_LC1', 'T_LC_Dry1', 'T_LC_Wet1', 'T_LC_Max1', 'RCC1', 'LCC1', 'canthiMax1', 'canthi4Max1', 'T_FHCC1', 'T_FHRC1', 'T_FHLC1', 'T_FHBC1', 'T_FHTC1', 'T_FH_Max1', 'T_FHC_Max1', 'T_Max1', 'T_OR1', 'T_OR_Max1']\n",
        "    lst6 = [col[:-1] + '2' for col in lst5]\n",
        "    lst7 = [col[:-1] + '3' for col in lst5]\n",
        "    lst8 =  [col[:-1] + '4' for col in lst5]\n",
        "\n",
        "    def column_change(lst1):\n",
        "      column1 = {}\n",
        "      for i in lst1:\n",
        "        if i[-1] in ('1','2','3','4'):\n",
        "          col1 = i[:-1]\n",
        "          column1[i] = col1\n",
        "\n",
        "      return column1\n",
        "\n",
        "    train = pd.read_csv(filename,skiprows=2)\n",
        "\n",
        "    col1 = []\n",
        "    for i in train.columns:\n",
        "      if train[i].isnull().all():\n",
        "        col1.append(i)\n",
        "\n",
        "    train1 = train.drop(columns = col1)\n",
        "    train1.rename(columns = {'Max1R13_1' : 'Max1R131', 'Max1L13_1':'Max1L131', 'aveAllR13_1':'aveAllR131', 'aveAllL13_1':'aveAllL131','Max1R13_2' : 'Max1R132', 'Max1L13_2':'Max1L132', 'aveAllR13_2':'aveAllR132', 'aveAllL13_2':'aveAllL132','Max1R13_3' : 'Max1R133', 'Max1L13_3':'Max1L133', 'aveAllR13_3':'aveAllR133', 'aveAllL13_3':'aveAllL133','Max1R13_4' : 'Max1R134', 'Max1L13_4':'Max1L134', 'aveAllR13_4':'aveAllR134', 'aveAllL13_4':'aveAllL134','T_LC14':'T_LC4'},inplace = True)\n",
        "\n",
        "    for col in lst5:\n",
        "      col1 = col[:-1]\n",
        "      # print(col1)\n",
        "      train1[col1] = (train1[col1 + '1'] + train1[col1 + '2'] + train1[col1 + '3'] + train1[col1 + '4'])/4\n",
        "\n",
        "    train1.drop(columns = lst5 + lst6+lst7+lst8 +['aveOralF'],inplace= True)\n",
        "    return train1\n",
        "\n",
        "  def feature_transformation(self,train1,mode,flir):\n",
        "\n",
        "    num = []\n",
        "    cat = []\n",
        "\n",
        "    for col in train1.columns:\n",
        "\n",
        "      if train1[col].dtype == np.float64:\n",
        "        num.append(col)\n",
        "      else:\n",
        "        cat.append(col)\n",
        "\n",
        "    for col in num:\n",
        "      train1[col].fillna(train1[col].median(),inplace = True)\n",
        "\n",
        "    # print(train1.head())\n",
        "\n",
        "    if mode == 'train':\n",
        "      train1 = train1[train1[\"Distance\"] < 1]\n",
        "      train1 = train1[train1[\"T_offset\"] < (7/4)]\n",
        "      train1 = train1[train1[\"T_offset\"] > (1/4)]\n",
        "      train1 = train1[train1[\"T_FHCC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHRC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHLC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHLC\"] < (145/4)]\n",
        "      train1 = train1[train1[\"T_FHBC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHTC\"] > (130/4)]\n",
        "      train1 = train1[train1[\"T_FHC_Max\"] > (132.5/4)]\n",
        "      train1 = train1[train1[\"T_FHC_Max\"] < (147.5/4)]\n",
        "      train1 = train1[train1[\"T_Max\"] < (150/4)]\n",
        "      train1.drop(columns = [\"T_offset\",\"T_atm\", \"Humidity\", \"Distance\"], inplace = True)\n",
        "\n",
        "    if mode == 'test':\n",
        "      if \"Cometics\" in train1.columns:\n",
        "        train1.drop(columns = ['Cosmetics'],inplace = True)\n",
        "\n",
        "    train1.reset_index(inplace = True)\n",
        "    train1.drop(columns = ['index'],inplace=True)\n",
        "    data_cat = train1[cat]\n",
        "\n",
        "    # data_cat.drop(columns = [\"SubjectID\"], inplace = True)\n",
        "    # if mode == 'train':\n",
        "    #   gender_dummies = pd.get_dummies(data_cat['Gender'])\n",
        "    #   self.gender_dict = {gender:1 for gender in gender_dummies.columns}\n",
        "\n",
        "    #   # print(gender_dummies.head())\n",
        "    #   gender_dummies['Female'] = gender_dummies['Female'].replace({True: 1, False: 0})\n",
        "    #   gender_dummies['Male'] = gender_dummies['Male'].replace({True: 1, False: 0})\n",
        "\n",
        "    #   age_dummies = pd.get_dummies(data_cat['Age'])\n",
        "    #   self.age_dict = {age:1 for age in age_dummies.columns}\n",
        "\n",
        "    #   for col in age_dummies:\n",
        "    #     age_dummies[col] = age_dummies[col].replace({True: 1, False: 0})\n",
        "\n",
        "    #   ethnicity_dummies = pd.get_dummies(data_cat['Ethnicity'])\n",
        "\n",
        "    #   for col in ethnicity_dummies:\n",
        "    #     ethnicity_dummies[col] = ethnicity_dummies[col].replace({True: 1, False: 0})\n",
        "\n",
        "    #   self.ethnicity_dict = {ethnicity:1 for ethnicity in ethnicity_dummies.columns}\n",
        "\n",
        "    #   gender_dummies = gender_dummies.join(age_dummies)\n",
        "    #   gender_dummies = gender_dummies.join(ethnicity_dummies)\n",
        "\n",
        "    #   train1 = train1.join(gender_dummies)\n",
        "    # else:\n",
        "    num1 = train1.copy(deep=True)\n",
        "\n",
        "    self.gender_dict = flir.gender_dict\n",
        "    # print(self.gender_dict)\n",
        "    for col in self.gender_dict:\n",
        "      values = []\n",
        "      for i in range(len(train1['Gender'])):\n",
        "        if train1['Gender'][i] == col:\n",
        "          values.append(1)\n",
        "        else:\n",
        "          values.append(0)\n",
        "      train1[col] = pd.Series(values)\n",
        "\n",
        "    self.age_dict = flir.age_dict\n",
        "    # print(self.age_dict)\n",
        "    for col in self.age_dict:\n",
        "      values = []\n",
        "      for i in range(len(train1['Age'])):\n",
        "        if train1['Age'][i] == col:\n",
        "          values.append(1)\n",
        "        else:\n",
        "          values.append(0)\n",
        "      train1[col] = pd.Series(values)\n",
        "\n",
        "    self.ethnicity_dict = flir.ethnicity_dict\n",
        "    for col in self.ethnicity_dict:\n",
        "      values = []\n",
        "      for i in range(len(train1['Ethnicity'])):\n",
        "        if train1['Ethnicity'][i] == col:\n",
        "          values.append(1)\n",
        "        else:\n",
        "          values.append(0)\n",
        "      train1[col] = pd.Series(values)\n",
        "\n",
        "\n",
        "    drop_col = [\"SubjectID\" , \"Gender\", \"Age\" , \"Ethnicity\",'Cosmetics', 'Time', 'Date']\n",
        "    train1.drop(columns = drop_col, inplace = True)\n",
        "    num1.drop(columns = drop_col,inplace = True)\n",
        "    return train1,num1\n"
      ],
      "metadata": {
        "id": "73phQn7XShUx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MODEL FITTING**"
      ],
      "metadata": {
        "id": "X_TYKFg60meT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fiting(data_num,num1):\n",
        "  # print(len(num1.columns))\n",
        "  # print(data_num.head())\n",
        "\n",
        "  def mae_loss(y_true, y_pred):\n",
        "    mae = np.abs(np.mean(y_pred - y_true) )\n",
        "    return mae\n",
        "\n",
        "  def mse_loss(y_true, y_pred):\n",
        "    mse = (np.mean(y_pred - y_true)**2)\n",
        "    return mse\n",
        "\n",
        "\n",
        "\n",
        "  mae_scorer = make_scorer(mae_loss, greater_is_better=False)\n",
        "  mse_scorer = make_scorer(mse_loss, greater_is_better=False)\n",
        "  # rmse_scorer = make_scorer(rmse_loss, greater_is_better=False)\n",
        "\n",
        "\n",
        "  def make_regression(x,y, model, model_name, feature_set_name, verbose=True):\n",
        "\n",
        "    if model_name == 'Trivial':\n",
        "      train_error = 0\n",
        "      val_error = 0\n",
        "      mae = 0\n",
        "      mse = 0\n",
        "      rmse = 0\n",
        "      kf = KFold(n_splits=10, random_state=None, shuffle=False)\n",
        "      for i, (train_index, val_index) in enumerate(kf.split(x)):\n",
        "        model1 = model\n",
        "        x_train,y_train = x.iloc[train_index],y.iloc[train_index]\n",
        "        x_val,y_val = x.iloc[val_index],y.iloc[val_index]\n",
        "\n",
        "        y_predict=[np.mean(train_data1['aveOralM']) for i in range(len(y_train))]\n",
        "        train_error += mean_squared_error(y_train, y_predict, squared=False)\n",
        "\n",
        "        y_predict = [np.mean(train_data1['aveOralM']) for i in range(len(y_val))]\n",
        "        val_error += mean_squared_error(y_val, y_predict, squared=False)\n",
        "        rmse += np.sqrt(mean_squared_error(y_val, y_predict, squared=False))\n",
        "        mse += mean_squared_error(y_val, y_predict, squared=False)\n",
        "        mae += mean_absolute_error(y_val,y_predict)\n",
        "\n",
        "\n",
        "        # mae = -cross_val_score(model, x_val, y_val, cv=10, scoring=mae_scorer).mean()\n",
        "        # rmse = np.sqrt(-cross_val_score(model, x_val, y_val, cv=10, scoring=mse_scorer)).mean()\n",
        "        # mse = -cross_val_score(model, x_val, y_val, cv=10, scoring=mse_scorer).mean()\n",
        "\n",
        "      if verbose:\n",
        "          print(\"----Model name = {}-----\".format(model_name))\n",
        "          print(\"Train error = \"'{}'.format(train_error/10))\n",
        "          print(\"val error = \"'{}'.format(val_error/10))\n",
        "          print(\"mae_score = \"'{}'.format(mae/10))\n",
        "          print(\"mse_score = \"'{}'.format(mse/10))\n",
        "          print(\"rmse_score = \"'{}'.format(rmse/10))\n",
        "          print(\"--------------------------------\")\n",
        "\n",
        "      trained_model = 1\n",
        "\n",
        "      return trained_model, y_predict, train_error/10, val_error/10, mae/10,mse/10,rmse/10\n",
        "    else:\n",
        "      os.mkdir('./models/'+model_name +'_'+feature_set_name)\n",
        "      train_error = 0\n",
        "      val_error = 0\n",
        "      mae = 0\n",
        "      mse = 0\n",
        "      rmse = 0\n",
        "      kf = KFold(n_splits=10, random_state=None, shuffle=False)\n",
        "      for i, (train_index, val_index) in enumerate(kf.split(x)):\n",
        "        model1 = model\n",
        "        x_train,y_train = x.iloc[train_index],y.iloc[train_index]\n",
        "        x_val,y_val = x.iloc[val_index],y.iloc[val_index]\n",
        "\n",
        "        model1.fit(x_train,y_train)\n",
        "        with open('./models/'+ model_name+ '_'+feature_set_name+'/'+ model_name + '_'+str(i) + '.pkl', 'wb') as f:\n",
        "          pickle.dump(model1, f)\n",
        "\n",
        "        y_predict=model1.predict(x_train)\n",
        "        train_error += mean_squared_error(y_train, y_predict, squared=False)\n",
        "\n",
        "        y_predict = model1.predict(x_val)\n",
        "        val_error += mean_squared_error(y_val, y_predict, squared=False)\n",
        "        rmse += np.sqrt(mean_squared_error(y_val, y_predict, squared=False))\n",
        "        mse += mean_squared_error(y_val, y_predict, squared=False)\n",
        "        mae += mean_absolute_error(y_val,y_predict)\n",
        "\n",
        "\n",
        "        # mae = -cross_val_score(model, x_val, y_val, cv=10, scoring=mae_scorer).mean()\n",
        "        # rmse = np.sqrt(-cross_val_score(model, x_val, y_val, cv=10, scoring=mse_scorer)).mean()\n",
        "        # mse = -cross_val_score(model, x_val, y_val, cv=10, scoring=mse_scorer).mean()\n",
        "\n",
        "      if verbose:\n",
        "          print(\"----Model name = {}-----\".format(model_name))\n",
        "          print(\"Train error = \"'{}'.format(train_error/10))\n",
        "          print(\"val error = \"'{}'.format(val_error/10))\n",
        "          print(\"mae_score = \"'{}'.format(mae/10))\n",
        "          print(\"mse_score = \"'{}'.format(mse/10))\n",
        "          print(\"rmse_score = \"'{}'.format(rmse/10))\n",
        "          print(\"--------------------------------\")\n",
        "\n",
        "      trained_model = model1\n",
        "\n",
        "      return trained_model, y_predict, train_error/10, val_error/10, mae/10,mse/10,rmse/10\n",
        "\n",
        "  feature_sets = {}\n",
        "\n",
        "  if 'Cosmetics' in data_num.columns:\n",
        "    # feature_sets[\"full_dataset\"] = data_num.drop(columns = [\"aveOralM\",'Cosmetics','Time','Date'])\n",
        "    feature_sets[\"numerical_dataset\"] = num1.drop(columns = [\"aveOralM\",'Cosmetics','Time','Date'])\n",
        "  else:\n",
        "    # feature_sets[\"full_dataset\"] = data_num.drop(columns = [\"aveOralM\"])\n",
        "    feature_sets[\"numerical_dataset\"] = num1.drop(columns = [\"aveOralM\"])\n",
        "\n",
        "  regression_models = {\n",
        "    \"Ridge\" : linear_model.Ridge(random_state = 42),\n",
        "    \"DecisionTree\" : tree.DecisionTreeRegressor(random_state = 42, max_depth=6),\n",
        "    \"RandomForest\" : RandomForestRegressor(random_state = 42),\n",
        "    \"XGBoost\": XGBRegressor(random_state = 42),\n",
        "    \"LGBM\": LGBMRegressor(random_state = 42),\n",
        "    \"MLP\":  MLPRegressor(random_state = 42),\n",
        "    \"Regression\" : LinearRegression(),\n",
        "    \"kNN\": KNeighborsRegressor(n_neighbors=1),\n",
        "    \"Trivial\":1,\n",
        "    }\n",
        "\n",
        "  pred_dict = {\n",
        "    \"regression_model\": [],\n",
        "    \"feature_set\": [],\n",
        "    \"Train Error\": [],\n",
        "    \"val Error\": [],\n",
        "    \"mae\" : [],\n",
        "    \"mse\" : [],\n",
        "    \"rmse\" : []\n",
        "    }\n",
        "\n",
        "  for feature_set_name in feature_sets.keys():\n",
        "    feature_set = feature_sets[feature_set_name]\n",
        "    print(\"Included columns are {}\".format(feature_set_name))\n",
        "    for model_name in regression_models.keys():\n",
        "\n",
        "          y = data_num[\"aveOralM\"]\n",
        "          x = feature_set\n",
        "          # x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "          trained_model, y_predict, train_error, val_error, mae,mse,rmse = make_regression(x,y, regression_models[model_name], model_name, feature_set_name,verbose=True)\n",
        "\n",
        "\n",
        "          pred_dict[\"regression_model\"].append(model_name)\n",
        "          pred_dict[\"feature_set\"].append(feature_set_name)\n",
        "          pred_dict[\"Train Error\"].append(train_error)\n",
        "          pred_dict[\"val Error\"].append(val_error)\n",
        "          pred_dict[\"mae\"].append(mae)\n",
        "          pred_dict[\"mse\"].append(mse)\n",
        "          pred_dict[\"rmse\"].append(rmse)\n",
        "\n",
        "  pred_df = pd.DataFrame(pred_dict)\n",
        "  pred_df[\"feature_set_2\"] = pred_df[\"feature_set\"].apply(lambda x: x.split('_')[0])\n",
        "  pred_df[\"Model_with_Data_set\"] = pred_df['regression_model'] +\"_\"+ pred_df[\"feature_set_2\"]\n",
        "  df_barh = pred_df[[\"Train Error\",\"val Error\", \"mae\",'mse','rmse', \"Model_with_Data_set\" ]]\n",
        "  df_train_error = df_barh[['Model_with_Data_set', 'Train Error']]\n",
        "  df_val_error = df_barh[['Model_with_Data_set', 'val Error']]\n",
        "\n",
        "  fig, (ax2, ax3, ax4,ax5,ax6) = plt.subplots(1, 5, figsize=(20, 6))\n",
        "\n",
        "  df_barh.plot(kind='barh', x='Model_with_Data_set', y='mae', color='red', ax=ax2, legend=False)\n",
        "  ax2.set_xlabel('MAE')\n",
        "  ax2.set_ylabel('Model')\n",
        "  ax2.set_title('MAE')\n",
        "\n",
        "  df_barh.plot(kind='barh', x='Model_with_Data_set', y='mse', color='yellow', ax=ax3, legend=False)\n",
        "  ax3.set_xlabel('MSE')\n",
        "  ax3.set_ylabel('Model')\n",
        "  ax3.set_title('MSE')\n",
        "\n",
        "  df_barh.plot(kind='barh', x='Model_with_Data_set', y='rmse', color='black', ax=ax4, legend=False)\n",
        "  ax4.set_xlabel('RMSE')\n",
        "  ax4.set_ylabel('Model')\n",
        "  ax4.set_title('RMSE')\n",
        "\n",
        "\n",
        "  df_train_error.plot(kind='barh', x='Model_with_Data_set', y='Train Error', color='blue', ax=ax5, legend=False)\n",
        "  ax5.set_xlabel('Train Error')\n",
        "  ax5.set_ylabel('Model')\n",
        "  ax5.set_title('Train Error')\n",
        "\n",
        "\n",
        "  df_val_error.plot(kind='barh', x='Model_with_Data_set', y='val Error', color='green', ax=ax6, legend=False)\n",
        "  ax6.set_xlabel('val Error')\n",
        "  ax6.set_ylabel('Model')\n",
        "  ax6.set_title('val Error')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  return pred_df"
      ],
      "metadata": {
        "id": "MrtQJ7gS10B1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**EVALUATE**"
      ],
      "metadata": {
        "id": "gcqTUMf60tXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(test_data):\n",
        "\n",
        "  def make_regressor(x_test, y_test, model_name,feature_set_name, verbose=True):\n",
        "    dict1 = {}\n",
        "    y_predict = 0\n",
        "    for name in os.listdir('./models/' + model_name +\"_\" +feature_set_name ):\n",
        "      model1 = pickle.load(open('./models/' + model_name +\"_\" +feature_set_name +'/' + name,'rb'))\n",
        "      # print(model1.feature_names_in_)\n",
        "      y_predict+=model1.predict(x_test)\n",
        "    y_predict/=10\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_predict)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
        "    mse = mean_squared_error(y_test, y_predict)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"----Model name = {}-----\".format(model_name))\n",
        "        print(\"mae_score = \"'{}'.format(mae))\n",
        "        print(\"mse_score = \"'{}'.format(mse))\n",
        "        print(\"rmse_score = \"'{}'.format(rmse))\n",
        "        print(\"--------------------------------\")\n",
        "\n",
        "    return mae,mse,rmse\n",
        "\n",
        "  models = os.listdir('./models/')\n",
        "  pred_dict = {\n",
        "    \"regression_model\": [],\n",
        "    \"mae\" : [],\n",
        "    \"mse\" : [],\n",
        "    \"rmse\" : []\n",
        "    }\n",
        "\n",
        "  if 'Time' in test_data.columns:\n",
        "    x_test = test_data.drop(columns = [\"aveOralM\"]) #, 'T_atm', 'Humidity', 'Distance','Time', 'Date','T_offset'])\n",
        "  else:\n",
        "    x_test = test_data.drop(columns = [\"aveOralM\"]) #, 'T_atm', 'Humidity', 'Distance','T_offset'])\n",
        "  y_test = test_data[\"aveOralM\"]\n",
        "  for model in models:\n",
        "    # print(model.split('_')[0])\n",
        "    mae,mse,rmse = make_regressor(x_test, y_test, model.split('_')[0],\"_\".join(model.split(\"_\")[1:]), verbose=True)\n",
        "\n",
        "    pred_dict[\"regression_model\"].append(model.split('.')[0])\n",
        "    pred_dict[\"mae\"].append(mae)\n",
        "    pred_dict[\"mse\"].append(mse)\n",
        "    pred_dict[\"rmse\"].append(rmse)\n",
        "\n",
        "  print(pred_dict)\n",
        "  pred_df = pd.DataFrame(pred_dict)\n",
        "  print(pred_df.head())\n",
        "  # pred_df[\"feature_set_2\"] = pred_df[\"feature_set\"].apply(lambda x: x.split('_')[0])\n",
        "  pred_df[\"Model_with_Data_set\"] = pred_df['regression_model']\n",
        "  df_barh = pred_df[[\"mae\",'mse','rmse', \"Model_with_Data_set\" ]]\n",
        "  # print(df_barh)\n",
        "\n",
        "  fig, (ax2, ax3, ax4) = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "  df_barh.plot(kind='barh', x='Model_with_Data_set', y='mae', color='red', ax=ax2, legend=False)\n",
        "  ax2.set_xlabel('MAE')\n",
        "  ax2.set_ylabel('Model')\n",
        "  ax2.set_title('MAE')\n",
        "\n",
        "  df_barh.plot(kind='barh', x='Model_with_Data_set', y='mse', color='yellow', ax=ax3, legend=False)\n",
        "  ax3.set_xlabel('MSE')\n",
        "  ax3.set_ylabel('Model')\n",
        "  ax3.set_title('MSE')\n",
        "\n",
        "  df_barh.plot(kind='barh', x='Model_with_Data_set', y='rmse', color='black', ax=ax4, legend=False)\n",
        "  ax4.set_xlabel('RMSE')\n",
        "  ax4.set_ylabel('Model')\n",
        "  ax4.set_title('RMSE')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  return pred_dict"
      ],
      "metadata": {
        "id": "mnRvmyKRjvB5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Original DATA**"
      ],
      "metadata": {
        "id": "_X7q6jYH_a3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == '__main__':\n",
        "FLIR = FLIR_dataset()\n",
        "data1 = FLIR.preprocessing('/content/FLIR_groups1and2_train.csv')\n",
        "# data2 = preprocessing('/content/facial-and-oral-temperature-data-from-a-large-set-of-human-subject-volunteers-1.0.0/ICI_groups1and2.csv')\n",
        "data3 = data1\n",
        "# print(data3.head())\n",
        "# print(train_data['T_FHCC'].describe())\n",
        "# print((train_data).head)\n",
        "train_data1,num1 = FLIR.feature_transformation(data3,'train')\n",
        "train_data1 = train_data1.reindex(sorted(train_data1.columns), axis=1)\n",
        "print(train_data1.columns)\n",
        "\n",
        "# print(len(train_data1))\n",
        "pred_dict = model_fiting(train_data1,num1)\n",
        "# print(len(train_data1))\n",
        "\n"
      ],
      "metadata": {
        "id": "UpKt0pTtxUgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = [np.mean(train_data1['aveOralM']) for i in range(len(train_data1))]\n",
        "y_train = train_data1['aveOralM']\n",
        "mae = mean_absolute_error(y_train, y_predict)\n",
        "rmse = np.sqrt(mean_squared_error(y_train, y_predict))\n",
        "mse = mean_squared_error(y_train, y_predict)"
      ],
      "metadata": {
        "id": "1qzOuqvY0_6t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mae)\n",
        "print(rmse)\n",
        "print(mse)\n"
      ],
      "metadata": {
        "id": "iKMs8MJ_Gdyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dict.drop(columns = ['feature_set','feature_set_2','Model_with_Data_set'])"
      ],
      "metadata": {
        "id": "UTtbBCxUMWR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**TESTING**"
      ],
      "metadata": {
        "id": "yf82lOBh07P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = FLIR.preprocessing('/content/FLIR_groups1and2_test.csv')\n",
        "# print(test_data.columns)\n",
        "test_data1,num1 = FLIR.feature_transformation(test_data,'test')\n",
        "test_data1 = test_data1.reindex(sorted(test_data1.columns), axis=1)\n",
        "col = ['Max1R13' ,'Max1L13', 'aveAllR13', 'aveAllL13', 'T_RC', 'T_RC_Dry', 'T_RC_Wet',\n",
        " 'T_RC_Max', 'T_LC', 'T_LC_Dry', 'T_LC_Wet', 'T_LC_Max', 'RCC' ,'LCC',\n",
        " 'canthiMax', 'canthi4Max' ,'T_FHCC', 'T_FHRC', 'T_FHLC', 'T_FHBC', 'T_FHTC',\n",
        " 'T_FH_Max', 'T_FHC_Max', 'T_Max', 'T_OR' ,'T_OR_Max','aveOralM']\n",
        "test_data1 = test_data1[col]\n",
        "# print(test_data1.columns)\n",
        "pred_dict = evaluate(test_data1)"
      ],
      "metadata": {
        "id": "nZzJ_VQkeEBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(test_data1.columns)"
      ],
      "metadata": {
        "id": "bvvrhs8W7AGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "263B1Xw1eDDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trivial Algorithm**"
      ],
      "metadata": {
        "id": "pYQS57bueEA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = [np.mean(train_data1['aveOralM']) for i in range(len(test_data1))]\n",
        "y_test = test_data['aveOralM']\n",
        "mae = mean_absolute_error(y_test, y_predict)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_predict))\n",
        "mse = mean_squared_error(y_test, y_predict)"
      ],
      "metadata": {
        "id": "woWf9QnVbccT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dict = pd.DataFrame(pred_dict)\n",
        "pred_dict"
      ],
      "metadata": {
        "id": "WtZVXH8qdOmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mae)\n",
        "print(mse)\n",
        "print(rmse)"
      ],
      "metadata": {
        "id": "-R3onkG1jtQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fqkUqkUoc7JR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**"
      ],
      "metadata": {
        "id": "GuJKWzq0PA7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FLIR = FLIR_dataset()\n",
        "data1 = FLIR.preprocessing('/content/FLIR_groups1and2_train.csv')\n",
        "train_data1,num1 = FLIR.feature_transformation(data1,'train')"
      ],
      "metadata": {
        "id": "1OS7yiXoQ2f5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_num(col):\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(20, 4))\n",
        "  plt.subplot(1,2,1)\n",
        "  sns.histplot(train_data1[col],kde=True)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.boxplot(train_data1[col])\n",
        "  plt.title(\"Box Plot of \" + col)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ljh6obMPPF1B"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_num('Max1R13')"
      ],
      "metadata": {
        "id": "3BW5MYRPPRY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_num(\"T_FHCC\")"
      ],
      "metadata": {
        "id": "1it2fSlw3Jfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_num(\"T_FHRC\")"
      ],
      "metadata": {
        "id": "pjJShFk73JjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_num(\"T_FHLC\")"
      ],
      "metadata": {
        "id": "c_FQD0Pv3Jln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_num(\"T_FHBC\")"
      ],
      "metadata": {
        "id": "kwYiPsxh3Jn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_num(\"T_FHTC\")"
      ],
      "metadata": {
        "id": "U7Sf3xeb3JqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_num(\"T_FHC_Max\")"
      ],
      "metadata": {
        "id": "r7r0Y9NE3Jsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_num(\"T_Max\")"
      ],
      "metadata": {
        "id": "IZrkbT783JvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Bivariate Analysis**"
      ],
      "metadata": {
        "id": "JAwLlbmI9U3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FLIR = FLIR_dataset()\n",
        "data1 = FLIR.preprocessing('/content/FLIR_groups1and2_train.csv')\n",
        "train_data1,num1 = FLIR.feature_transformation(data1,'train')"
      ],
      "metadata": {
        "id": "9hNPLSR09Zu7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bivariate_analysis(input,output):\n",
        "\n",
        "  x,y = np.array(train_data1[input]).reshape(-1,1), np.array(train_data1[output]).reshape(-1,1)\n",
        "  data1 = pd.DataFrame(np.concatenate([x,y],axis = 1))\n",
        "  data1.dropna(inplace=True)\n",
        "  # print(data1.isnull().sum())\n",
        "  # print(data1.head())\n",
        "  # print(data1.columns)\n",
        "  x = np.array(data1[[0]]).reshape(-1,1)\n",
        "  y = np.array(data1[[1]]).reshape(-1,1)\n",
        "  my_pwlf = LinearRegression()\n",
        "  my_pwlf.fit(x,y)\n",
        "  # res = my_pwlf.fit(4)\n",
        "\n",
        "  xHat = np.linspace(min(x), max(x), num=10000)\n",
        "  yHat = my_pwlf.predict(xHat)\n",
        "\n",
        "  y1 = my_pwlf.predict(x)\n",
        "  # plot the results\n",
        "  plt.figure()\n",
        "  plt.plot(x, y, 'o')\n",
        "  plt.plot(xHat, yHat, '-')\n",
        "  plt.xlabel(input)\n",
        "  plt.ylabel(output)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "EvxmM27WCyEp"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('Max1R13','aveOralM')"
      ],
      "metadata": {
        "id": "KyKO_cBXUDW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('Max1L13','aveOralM')"
      ],
      "metadata": {
        "id": "X4ZAh3y_UYac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('aveAllR13','aveOralM')"
      ],
      "metadata": {
        "id": "JlVdmAXRUkyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('aveAllL13','aveOralM')"
      ],
      "metadata": {
        "id": "A9Flsp-fUk0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_RC','aveOralM')"
      ],
      "metadata": {
        "id": "m4lhqZ2vUrsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_RC_Dry','aveOralM')"
      ],
      "metadata": {
        "id": "KvhV-21WVAj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_RC_Wet','aveOralM')"
      ],
      "metadata": {
        "id": "PJJRuoMfVJmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_RC_Max','aveOralM')"
      ],
      "metadata": {
        "id": "OKckOj4GVJoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_LC_Dry','aveOralM')"
      ],
      "metadata": {
        "id": "67FLz5oaVJqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_LC_Wet','aveOralM')"
      ],
      "metadata": {
        "id": "_YmHoQ2qVJs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_LC_Max','aveOralM')"
      ],
      "metadata": {
        "id": "JZmrnlAbVJve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('RCC','aveOralM')"
      ],
      "metadata": {
        "id": "Oso8YktSVJxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('LCC','aveOralM')"
      ],
      "metadata": {
        "id": "MWPLIXVbV25D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('canthiMax','aveOralM')"
      ],
      "metadata": {
        "id": "-oMXasWgV27s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('canthi4Max','aveOralM')"
      ],
      "metadata": {
        "id": "R48Lt9YwV29b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_FHCC','aveOralM')"
      ],
      "metadata": {
        "id": "qCBioqv7V2_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_FHRC','aveOralM')"
      ],
      "metadata": {
        "id": "I1k5SRd5V_bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_FHLC','aveOralM')"
      ],
      "metadata": {
        "id": "UxGmCCpnV_m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_FHBC','aveOralM')"
      ],
      "metadata": {
        "id": "rPu9js05WAAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_FHTC','aveOralM')"
      ],
      "metadata": {
        "id": "2f_NxMuAWAFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_FH_Max','aveOralM')"
      ],
      "metadata": {
        "id": "aDvxxzj-WAHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_FHC_Max','aveOralM')"
      ],
      "metadata": {
        "id": "SjfFx_24WAJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_Max','aveOralM')"
      ],
      "metadata": {
        "id": "VGLNl8KQWTj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_OR','aveOralM')"
      ],
      "metadata": {
        "id": "6Kg2K-HOWTmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bivariate_analysis('T_OR_Max','aveOralM')"
      ],
      "metadata": {
        "id": "11rVpXq0WTod"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}